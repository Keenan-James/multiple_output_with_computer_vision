{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/dc-aihub/dc-aihub.github.io/blob/master/img/ai-logo-transparent-banner.png?raw=true\" \n",
    "alt=\"Ai/Hub Logo\"/>\n",
    "\n",
    "<h1 style=\"text-align:center;color:#0B8261;\"><center>Artificial Intelligence</center></h1>\n",
    "<h1 style=\"text-align:center;\"><center>Part 1 - Planes, Trains, an Automobiles: </center></h1>\n",
    "<h1 style=\"text-align:center;\"><center>Multiple Classification Exercise with Multi-Binarized-Labels</center></h1>\n",
    "\n",
    "<center>***Original Tutorial by Adrian Rosebrock:*** <br/>https://www.pyimagesearch.com/2018/05/07/multi-label-classification-with-keras/</center>\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<center><a href=\"#OVERVIEW\">Overview</a></center>\n",
    "<center><a href=\"#BEFORE-YOU-BEGIN\">Before You Begin</a></center>\n",
    "<center><a href=\"#BUILD-THE-MODEL\">Build the Model</a></center>\n",
    "<center><a href=\"#IMAGE-PREPROCESSING\">Image Pre-Processing</a></center>\n",
    "<center><a href=\"#LABEL-BINARIZATION\">Label Binarization</a></center>\n",
    "<center><a href=\"#TRAIN-THE-MODEL\">Train the Model</a></center>\n",
    "<center><a href=\"#ACCURACY-STATISTICS\">Accuracy Statistics</a></center>\n",
    "<center><a href=\"#IMPLEMENTATION\">Implementation</a></center>\n",
    "<center><a href=\"#CONCLUSION\">Conclusion</a></center>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"OVERVIEW\">\n",
    "OVERVIEW\n",
    "</div>\n",
    "\n",
    "<center style=\"color:#0B8261;\">\n",
    "This exercise will aim to answer the question of how to generate multiple output classifications given a single input image using Convolutional Neural Networks. Two distinct solutions to this problem will be tested in order to compare their performance and analyze their merits. The first example will use a single model that will produce multiple outputs; essentially we will be two-hot encoding our training input and two-hot-decoding its output to identify multiple classes. The second model will be built with multiple branches that will handle the classification of each category independently with their own respective loss functions and output.\n",
    "</center>\n",
    "<br/>\n",
    "<center style=\"color:#0B8261;\">\n",
    "This exercise closely follows the tutorial series written by Adrian Rosebrock however we have substituted our own dataset that contains images of various vehicle types and colours. The models we will build will be tasked with learning, and then predicting, both the type and colour of an input image. The classes that have been chosen are shown in the directory tree below:\n",
    "    \n",
    "![](images/data-tree.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "It is important to note that not all possible combinations of classes are present in our dataset. For example we will want our models to identify the colour blue and also identify a plane, but we have not shown it a 'blue train' specifically in our training data. This will become important later as our main test to evaluate the performance of the model will revolve around seeing if they can correctly classify an object it has not explicitly seen.  \n",
    "\n",
    "The images were collected using Bing's Search API v7.0 that can be accessed using a Microsoft Azure account or by temporary free trail. More information on how to scrape images for Bing Search can be found [here](https://www.pyimagesearch.com/2018/04/09/how-to-quickly-build-a-deep-learning-image-dataset/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"BEFORE-YOU-BEGIN\">\n",
    "BEFORE YOU BEGIN\n",
    "</div>\n",
    "\n",
    "<center style=\"color:#0B8261;\">\n",
    "Ensure that you have the proper python packages installed. You will need the following:\n",
    "</center>\n",
    "<br/>\n",
    "<center>\n",
    "Keras will help us build our neural network.<br/>\n",
    "    \n",
    "[Keras](https://anaconda.org/conda-forge/keras)<br/>\n",
    "\n",
    "Numpy will allow us to convert our training input to two-hot-encoded arrays.<br/>\n",
    "\n",
    "[Numpy](https://anaconda.org/anaconda/numpy)<br/>\n",
    "\n",
    "SciKitLearn will binirize our labels specifically for multi-label output.<br/>\n",
    "\n",
    "[SciKitLearn](https://anaconda.org/anaconda/scikit-learn)<br/>\n",
    "\n",
    "Tools in OpenCV will help us pre-process our images for input to the model.<br/>\n",
    "\n",
    "[OpenCV](https://anaconda.org/conda-forge/opencv)<br/>\n",
    "\n",
    "By using argparse we will be able interpret the output of our model.<br/>\n",
    "\n",
    "[Argparse](https://anaconda.org/anaconda/argparse)<br/>\n",
    "\n",
    "Resources from Matplotlib will help us visualize the performance of our model over the various training epochs.<br/>\n",
    "\n",
    "[Matplotlib](https://anaconda.org/conda-forge/matplotlib)<br/>\n",
    "\n",
    "The Imutils library will help us overlay our predictions in text onto the image that was tested.<br/>\n",
    "\n",
    "[Imutils](https://anaconda.org/mlgill/imutils)<br/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"BUILD-THE-MODEL\">\n",
    "BUILD THE MODEL\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture of our convolutional neural network is based off one called “SmallerVGGNet” a lighter version of VGGNet first introduced in a 2014 paper by Simonyan and Zisserman. You can learn more about VGGNet and its inner workings by reading this post also written by Adrian Roseback and can be found [here](https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/).\n",
    "\n",
    "Our first approach will train a dedicated model on a single array that will represent each of our 8 different categorical possibilities of both types.  For example if our input array corresponded to [automobile, black, blue, plane, red, train, white, yellow] we can use Two-hot-encoding to denote a white automobile as [1,0,0,0,0,0,1,0].\n",
    "\n",
    "Once trained, the final layer of our model will output a probability matrix for the 8 classes with a single array. We can then take the two highest values of that array (Two-Hot-Decoding) in order to determine the type and colour of the pictured vehicle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hermione\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will create a class to build the SmallerVGGNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallerVGGNet:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, finalAct=\"softmax\"):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\t\tmodel.add(Conv2D(32, (3, 3), padding=\"same\",\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "        \n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    " \n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "\t\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\t\tmodel.add(Dropout(0.25))\n",
    "        \n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(1024))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\n",
    "\t\tmodel.add(Dense(classes))\n",
    "\t\tmodel.add(Activation(finalAct))\n",
    " \n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a method that can instantiate a model with 5 parameters; width, height, depth, classes, and final activation. Width and Height will be the dimensions in pixels of the input image. Depth represents the number of channels, which will be 3 handle our RBG input image.\n",
    "\n",
    "Classes will be the number of categories we have in our dataset. In this case it will be 8 classes for our types of vehicles and colour possibilities combined.\n",
    "\n",
    "Although the final activation is by default set to \"softmax\" if we change its value to \"sigmoid\" on instantiation it will enable Keras to perform multi-label classification for our output.\n",
    "\n",
    "The model itself consists of multiple layers following a similar pattern. 2D convolution layers with ‘relu’ activation that doubles its filter size on each instantiation to allow for increased abstraction. Dropout is used to randomly disconnect nodes leading to the next layer that will work to prevent over-fitting our model.  \n",
    "\n",
    "We will start by importing necessary packages; we will be using the [Keras API](https://keras.io/) to build and train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue by pre-processing image data and binarizing our labels/categories. We start by importing the necessary packages. We will also set matplotlib backend so that our figures are saved in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will initialize important variables to train the model. Tweak these variables to modify its performance and how long it will train for. 20 Epochs should be enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "INIT_LR = 1e-3\n",
    "BATCH_SIZE = 30\n",
    "IMAGE_DIMENSIONS = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Size refer to the number of training examples that will be used in one iteration. Image dimensions will match the input shape defined in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also initialize some constants to hold the folder and file paths in our working directory. The model and binarized class labels will be stored in the relative output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FOLDER = \"data/\"\n",
    "OUTPUT_FOLDER = \"multi_label_output/\"\n",
    "MODEL_FILE = \"vehicle_classification.model\"\n",
    "LABELS_FILE = \"labels.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we initialize some variables to handle image processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"IMAGE-PREPROCESSING\">\n",
    "IMAGE PRE-PROCESSING\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to cycle through all image file paths within our input data folder and add them to a list. The list of file paths is then shuffled randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_, _, files in os.walk(INPUT_DATA_FOLDER):\n",
    "    for fileName in files:\n",
    "        relDir = os.path.relpath(dir_, INPUT_DATA_FOLDER)\n",
    "        relFile = os.path.join(relDir, fileName)\n",
    "        if fileName is not None:\n",
    "            imagePaths.append(relFile)\n",
    "           \n",
    "\n",
    "random.seed(43)\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every file path in our list we will load the image, pre-process it for input, and add it to the data[] array. \n",
    "\n",
    "The classes will be determined by the folder path of the given image. By splitting the folder name we can extract its correct classification and add it to the labels array.\n",
    "\n",
    "We will end up with two arrays where the labels at labels[i] will correctly identify the image stored in data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagePath in imagePaths:\n",
    "        imagePath = INPUT_DATA_FOLDER + imagePath         \n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        if  image is not None:        \n",
    "            image = cv2.resize(image, (IMAGE_DIMENSIONS[1], IMAGE_DIMENSIONS[0]))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "         \n",
    "            l = imagePath.split(os.path.sep)[-2].split(\"_\")\n",
    "            labels.append(l)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is pre-processed above by resizing it to the dimensions necessitated as input by out model. We will also convert the image to an array using a Scitkit-learn method.\n",
    "\n",
    "We can now print an element in both our arrays to confirm that the labels are being saved accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\white_plane\\00000380.jpg\n",
      "['white', 'plane']\n"
     ]
    }
   ],
   "source": [
    "print(INPUT_DATA_FOLDER + imagePaths[1])\n",
    "print(labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"LABEL-BINARIZATION\">\n",
    "LABEL BINARIZATION\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using numpy we will convert the labels to an array and then binarize them with Scikit-learn. This is a crucial step in this tutorial as in order to perform a multiple classification problem we must use Scikit-learn's MultiLabelBinarizer which will transform our human readable labels into a vector that two hot encodes the classes in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] class labels:\n",
      "1. automobile\n",
      "2. black\n",
      "3. blue\n",
      "4. plane\n",
      "5. red\n",
      "6. train\n",
      "7. white\n",
      "8. yellow\n"
     ]
    }
   ],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1]\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels) \n",
    "\n",
    "print(\"[INFO] class labels:\")\n",
    "multiLabelBinarizer = MultiLabelBinarizer()\n",
    "labels = multiLabelBinarizer.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the array of our classes and also how many images our model will be trained on in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data matrix: 2752 images (594.43MB)\n"
     ]
    }
   ],
   "source": [
    "for (i, label) in enumerate(multiLabelBinarizer.classes_):\n",
    "\tprint(\"{}. {}\".format(i + 1, label))\n",
    "\n",
    "print(\"[INFO] data matrix: {} images ({:.2f}MB)\".format(\n",
    "\tlen(imagePaths), data.nbytes / (1024 * 1000.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"TRAIN-THE-MODEL\">\n",
    "TRAIN THE MODEL\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to split the data into training and testing segments. In the example 80% of the data will be used for training as denoted by the test_size parameter.\n",
    "\n",
    "We will also initialize a data augmenter object that is recommended practice for datasets with under 100 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "\tlabels, test_size=0.2, random_state=42)\n",
    " \n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n",
    "\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n",
    "\thorizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build the model and initialize the optimizer. The model is constructed with the parameters that were discussed earlier and an image of size 96 x 96 pixels, 3 colour channels to account for RGB, the numbers of classes in our dataset, and be sure to set the final activation function to \"sigmoid\" for our multiple-classification example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    }
   ],
   "source": [
    "model = SmallerVGGNet.build(\n",
    "\twidth=IMAGE_DIMENSIONS[1], height=IMAGE_DIMENSIONS[0],\n",
    "\tdepth=IMAGE_DIMENSIONS[2], classes=len(multiLabelBinarizer.classes_),\n",
    "\tfinalAct=\"sigmoid\")\n",
    " \n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to compile the model and start it's training using the fit method. For this example we will use binary cross-entropy in order to treat each output label as an independent Bernoulli distribution. A progress bar will indicate the passage of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/50\n",
      "73/73 [==============================] - 8s 109ms/step - loss: 0.5570 - acc: 0.7571 - val_loss: 0.5467 - val_acc: 0.8033\n",
      "Epoch 2/50\n",
      "73/73 [==============================] - 6s 76ms/step - loss: 0.2962 - acc: 0.8848 - val_loss: 0.4452 - val_acc: 0.8591\n",
      "Epoch 3/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.2525 - acc: 0.9023 - val_loss: 0.3855 - val_acc: 0.8879\n",
      "Epoch 4/50\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.2297 - acc: 0.9120 - val_loss: 0.3487 - val_acc: 0.9086\n",
      "Epoch 5/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.2211 - acc: 0.9177 - val_loss: 0.3172 - val_acc: 0.8988\n",
      "Epoch 6/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.1952 - acc: 0.9257 - val_loss: 0.4817 - val_acc: 0.8768\n",
      "Epoch 7/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.1825 - acc: 0.9293 - val_loss: 0.3407 - val_acc: 0.9002\n",
      "Epoch 8/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.1790 - acc: 0.9321 - val_loss: 0.2302 - val_acc: 0.9263\n",
      "Epoch 9/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.1680 - acc: 0.9363 - val_loss: 0.2046 - val_acc: 0.9338\n",
      "Epoch 10/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.1562 - acc: 0.9422 - val_loss: 0.2913 - val_acc: 0.9095\n",
      "Epoch 11/50\n",
      "73/73 [==============================] - 5s 70ms/step - loss: 0.1507 - acc: 0.9441 - val_loss: 0.1448 - val_acc: 0.9496\n",
      "Epoch 12/50\n",
      "73/73 [==============================] - 5s 70ms/step - loss: 0.1436 - acc: 0.9467 - val_loss: 0.2293 - val_acc: 0.9283\n",
      "Epoch 13/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.1458 - acc: 0.9452 - val_loss: 0.1980 - val_acc: 0.9285\n",
      "Epoch 14/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.1306 - acc: 0.9519 - val_loss: 0.1391 - val_acc: 0.9515\n",
      "Epoch 15/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.1259 - acc: 0.9532 - val_loss: 0.1418 - val_acc: 0.9562\n",
      "Epoch 16/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.1253 - acc: 0.9529 - val_loss: 0.1461 - val_acc: 0.9515\n",
      "Epoch 17/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.1220 - acc: 0.9543 - val_loss: 0.1997 - val_acc: 0.9426\n",
      "Epoch 18/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.1133 - acc: 0.9582 - val_loss: 0.2217 - val_acc: 0.9440\n",
      "Epoch 19/50\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.1126 - acc: 0.9589 - val_loss: 0.1921 - val_acc: 0.9419\n",
      "Epoch 20/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.1058 - acc: 0.9579 - val_loss: 0.1334 - val_acc: 0.9549\n",
      "Epoch 21/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.1054 - acc: 0.9609 - val_loss: 0.2348 - val_acc: 0.9270\n",
      "Epoch 22/50\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.0985 - acc: 0.9636 - val_loss: 0.1684 - val_acc: 0.9478\n",
      "Epoch 23/50\n",
      "73/73 [==============================] - 5s 70ms/step - loss: 0.1132 - acc: 0.9572 - val_loss: 0.2243 - val_acc: 0.9335\n",
      "Epoch 24/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0911 - acc: 0.9672 - val_loss: 0.1668 - val_acc: 0.9505\n",
      "Epoch 25/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.0931 - acc: 0.9665 - val_loss: 0.3314 - val_acc: 0.9145\n",
      "Epoch 26/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0895 - acc: 0.9656 - val_loss: 0.2018 - val_acc: 0.9399\n",
      "Epoch 27/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0946 - acc: 0.9634 - val_loss: 0.2603 - val_acc: 0.9265\n",
      "Epoch 28/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.0829 - acc: 0.9690 - val_loss: 0.1228 - val_acc: 0.9612\n",
      "Epoch 29/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0748 - acc: 0.9716 - val_loss: 0.1641 - val_acc: 0.9449\n",
      "Epoch 30/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.0884 - acc: 0.9668 - val_loss: 0.1518 - val_acc: 0.9519\n",
      "Epoch 31/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.0906 - acc: 0.9646 - val_loss: 0.1502 - val_acc: 0.9553\n",
      "Epoch 32/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0739 - acc: 0.9742 - val_loss: 0.1316 - val_acc: 0.9574\n",
      "Epoch 33/50\n",
      "73/73 [==============================] - 5s 75ms/step - loss: 0.0795 - acc: 0.9702 - val_loss: 0.2611 - val_acc: 0.9285\n",
      "Epoch 34/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.0649 - acc: 0.9761 - val_loss: 0.1574 - val_acc: 0.9544\n",
      "Epoch 35/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.0721 - acc: 0.9735 - val_loss: 0.1848 - val_acc: 0.9485\n",
      "Epoch 36/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.0705 - acc: 0.9752 - val_loss: 0.1451 - val_acc: 0.9583\n",
      "Epoch 37/50\n",
      "73/73 [==============================] - 5s 70ms/step - loss: 0.0871 - acc: 0.9680 - val_loss: 0.1897 - val_acc: 0.9456\n",
      "Epoch 38/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.0895 - acc: 0.9680 - val_loss: 0.2088 - val_acc: 0.9344\n",
      "Epoch 39/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.0772 - acc: 0.9711 - val_loss: 0.1342 - val_acc: 0.9605\n",
      "Epoch 40/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.0674 - acc: 0.9754 - val_loss: 0.1145 - val_acc: 0.9637\n",
      "Epoch 41/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0617 - acc: 0.9777 - val_loss: 0.1142 - val_acc: 0.9653\n",
      "Epoch 42/50\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.0599 - acc: 0.9786 - val_loss: 0.1383 - val_acc: 0.9583\n",
      "Epoch 43/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0587 - acc: 0.9798 - val_loss: 0.1647 - val_acc: 0.9505\n",
      "Epoch 44/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0538 - acc: 0.9799 - val_loss: 0.1268 - val_acc: 0.9662\n",
      "Epoch 45/50\n",
      "73/73 [==============================] - 5s 72ms/step - loss: 0.0624 - acc: 0.9778 - val_loss: 0.1265 - val_acc: 0.9642\n",
      "Epoch 46/50\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.0612 - acc: 0.9775 - val_loss: 0.1170 - val_acc: 0.9653\n",
      "Epoch 47/50\n",
      "73/73 [==============================] - 5s 74ms/step - loss: 0.0503 - acc: 0.9815 - val_loss: 0.1115 - val_acc: 0.9685\n",
      "Epoch 48/50\n",
      "73/73 [==============================] - 5s 73ms/step - loss: 0.0588 - acc: 0.9802 - val_loss: 0.2974 - val_acc: 0.9279\n",
      "Epoch 49/50\n",
      "73/73 [==============================] - 5s 71ms/step - loss: 0.0768 - acc: 0.9736 - val_loss: 0.2689 - val_acc: 0.9306\n",
      "Epoch 50/50\n",
      "73/73 [==============================] - 5s 70ms/step - loss: 0.0655 - acc: 0.9760 - val_loss: 0.1890 - val_acc: 0.9471\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "\n",
    "H = model.fit_generator(\n",
    "\taug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
    "\tvalidation_data=(testX, testY),\n",
    "\tsteps_per_epoch=len(trainX) // BATCH_SIZE,\n",
    "\tepochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then save the model to our output folder as well as a pickle file to hold the binarized labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network...\n",
      "[INFO] serializing label binarizer...\n"
     ]
    }
   ],
   "source": [
    "model.save(OUTPUT_FOLDER + MODEL_FILE)\n",
    " \n",
    "f = open(OUTPUT_FOLDER + LABELS_FILE, \"wb\")\n",
    "f.write(pickle.dumps(multiLabelBinarizer))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"ACCURACY-STATISTICS\">\n",
    "ACCURACY STATISTICS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visualize the training accuracy of our model over each epoch we can use matplotlib plot a graph and save it to the output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3c7de011869b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# plot the training loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train_loss\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = EPOCHS\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER + \"{}_accs.png\".format(\"output\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"IMPLEMENTATION\">\n",
    "IMPLEMENTATION\n",
    "</div>\n",
    "\n",
    "Once our model has been trained we can now feed it new images for classification and analyze its performance.\n",
    "\n",
    "We will start by importing the necessary packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize constants to handle file and folder names in our working directory. For each image we classify with the model we will overlay the prediction onto the image and then save it to the appropriate folder depending if it has been correctly classified or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FOLDER = \"unseen_class_combinations/\"\n",
    "RESULTS_FOLDERS = [\"correct_predictions/\", \"incorrect_predictions/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will reset variables that were used previously for training that will now be used to feed test images to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "imagePaths = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to when we trained our model we will loop over every file within the given directory and add its file path to an array and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir_, _, files in os.walk(INPUT_DATA_FOLDER):\n",
    "    for fileName in files:\n",
    "        relDir = os.path.relpath(dir_, INPUT_DATA_FOLDER)\n",
    "        relFile = os.path.join(relDir, fileName)\n",
    "        if fileName is not None:\n",
    "            imagePaths.append(relFile)\n",
    "           \n",
    "        \n",
    "random.seed(43)\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every image stored in our file-paths array will pre-process the image and keep track of its true classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagePath in imagePaths:\n",
    "        imagePath = INPUT_DATA_FOLDER + imagePath         \n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        if  image is not None: \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (IMAGE_DIMENSIONS[1], IMAGE_DIMENSIONS[0]))\n",
    "            image = img_to_array(image)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            data.append(image)\n",
    "            labels.append(imagePath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to load the model in addition to the binarized labels that were stored in our output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(OUTPUT_FOLDER + MODEL_FILE)    \n",
    "mlb = pickle.loads(open(OUTPUT_FOLDER + LABELS_FILE, \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake neatness we will clear all files that are currently stored in the correct and incorrect predictions folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in RESULTS_FOLDERS:\n",
    "\tfor the_file in os.listdir(OUTPUT_FOLDER + i):\n",
    "\t    file_path = os.path.join(OUTPUT_FOLDER + i, the_file)\n",
    "\t    try:\n",
    "\t        if os.path.isfile(file_path):\n",
    "\t            os.unlink(file_path)\n",
    "\t    except Exception as e:\n",
    "\t        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will cycle through all the images we want to classify in our chosen input folder and feed them to the model using its Predict() method. \n",
    "\n",
    "Subsequently we will overlay the prediction of the model onto the image and save it to the appropriate folder if it was successfully classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for (images, lab) in zip(data, labels):\n",
    "    proba = model.predict(images)[0]\n",
    "    idxs = np.argsort(proba)[::-1][:2]\n",
    " \n",
    "    categoryLabel = mlb.classes_[idxs[0]]\n",
    "    colorLabel = mlb.classes_[idxs[1]]\n",
    "\n",
    "    image = cv2.imread(lab)\n",
    "    \n",
    "    output = imutils.resize(image, width=1000)\n",
    "\n",
    "    cv2.putText(output, categoryLabel, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(output, colorLabel, (10, 55), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7, (0, 255, 0), 2)\n",
    "\n",
    "    (actual_colour, actual_category) = lab.split(os.path.sep)[-2].split(\"_\")\n",
    "\n",
    "    if actual_category == categoryLabel:\n",
    "        if actual_colour == colorLabel:\n",
    "            cv2.imwrite(OUTPUT_FOLDER + RESULTS_FOLDERS[0] + str(counter) + '.jpg', output)\n",
    "        elif actual_colour != colorLabel:\n",
    "            cv2.imwrite(OUTPUT_FOLDER + RESULTS_FOLDERS[1]+ str(counter) + '.jpg', output)\n",
    "    elif actual_category != categoryLabel:\n",
    "        cv2.imwrite(OUTPUT_FOLDER + RESULTS_FOLDERS[1]+ str(counter) + '.jpg', output)\n",
    "    \n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we count the number of images that were correctly identified by examining the images saved in its folder. A figure for the overall accuracy on our test set of images can also be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = len(data)\n",
    "\n",
    "list = os.listdir(OUTPUT_FOLDER + RESULTS_FOLDERS[0]) \n",
    "number_files = len(list)\n",
    "\n",
    "unseen_accuracy =  number_files/total_files * 100\n",
    "\n",
    "print ('The prediction accuracy for unseen combinations: %' + str(unseen_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"CONCLUSION\">\n",
    "CONCLUSION\n",
    "</div>\n",
    "\n",
    "Remember that the images we used to implement this model specifically contained objects whose class combinations had not been seen during training. This could explain why the model performs so poorly on our test set of images compared to the validation accuracy we saw during training. The take away for this exercise is to understand a fundamental flaw to the multi-binarized-label approach to the multiple output problem; that is that the trained model cannot make predictions for each class independently and therefore needs to be trained on all possible combinations of classes that you would desire to see as possible predictions. \n",
    "\n",
    "A better approach may be to create a model with multiple branches with independent layer architecture for each class; both for colour and vehicle type. We will test this approach in the next exercise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
