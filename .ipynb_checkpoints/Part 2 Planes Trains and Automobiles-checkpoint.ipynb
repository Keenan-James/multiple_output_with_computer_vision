{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"https://github.com/dc-aihub/dc-aihub.github.io/blob/master/img/ai-logo-transparent-banner.png?raw=true\" \n",
    "alt=\"Ai/Hub Logo\"/>\n",
    "\n",
    "<h1 style=\"text-align:center;color:#0B8261;\"><center>Artificial Intelligence</center></h1>\n",
    "<h1 style=\"text-align:center;\"><center>Part 2: Planes Trains & Automobiles:</center></h1>\n",
    "<h1 style=\"text-align:center;\"><center>Multiple Classification Exercise with Multi-Branch-Architecture</center></h1>\n",
    "\n",
    "<center>***Original Tutorial by Adrian Rosebrock:*** <br/>https://www.pyimagesearch.com/2018/06/04/keras-multiple-outputs-and-multiple-losses/</center>\n",
    "\n",
    "<hr/>\n",
    "<center><a href=\"#OVERVIEW\">Overview</a></center>\n",
    "<center><a href=\"#BUILD-THE-MODEL\">Build the Model</a></center>\n",
    "<center><a href=\"#IMAGE-PREPROCESSING\">Image Pre-Processing</a></center>\n",
    "<center><a href=\"#LABEL-BINARIZATION\">Label Binarization</a></center>\n",
    "<center><a href=\"#TRAIN-THE-MODEL\">Train the Model</a></center>\n",
    "<center><a href=\"#ACCURACY-STATISTICS\">Accuracy Statistics</a></center>\n",
    "<center><a href=\"#IMPLEMENTATION\">Implementation</a></center>\n",
    "<center><a href=\"#CONCLUSION\">Conclusion</a></center>\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"OVERVIEW\">\n",
    "OVERVIEW\n",
    "</div>\n",
    "\n",
    "In this exercise we will test a more advanced approach to the multiple classification problem. Essentially we will create a separate branch of our neural network for each class label in our dataset. This branch will have its own layer architecture and perform its respective set of convolution, activation, batch normalization, pooling and dropout functions resulting in its own independent output. The diagram below outlines each branch, note that it accepts a single input image of shape (96 x 96 x 3). \n",
    "\n",
    "![](images/loss-diagram-top.png)\n",
    "\n",
    "![](images/loss-diagram-bottom.png)\n",
    "\n",
    "\n",
    "Once again we will use the same dataset as our last exercise that contain 5 possible colour choices and 3 vehicle types; planes, trains, and automobiles.\n",
    "\n",
    "![](images/data-tree.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"BUILD-THE-MODEL\">\n",
    "BUILD THE MODEL\n",
    "</div>\n",
    "\n",
    "We will start by importing the necessary packages to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hermione\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we will build two sub networks with two separate functions. The category branch will determine the vehicle type followed by the colour branch.\n",
    "\n",
    "Note that our category branch will convert the input picture to grey scale using a Lambda layer. This is done because colour information is not needed by our model in order to make the distinction between vehicle types. The layer architecture itself follows a previous pattern as our first example. Convolutional 2D laver with relu activation, batch normalization, max pooling, and dropout. Again more information on the SmallerVGGNet can be found [here](https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VehicleNet:\n",
    "\t@staticmethod\n",
    "\tdef build_category_branch(inputs, numCategories,\n",
    "\t\tfinalAct=\"softmax\", chanDim=-1):\n",
    "\n",
    "\t\tx = Lambda(lambda c: tf.image.rgb_to_grayscale(c))(inputs)\n",
    " \n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "        \n",
    "\t\tx = Conv2D(64, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    " \n",
    "\t\tx = Conv2D(128, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "              \n",
    "\t\tx = Conv2D(256, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "        \n",
    "\n",
    "\t\tx = Conv2D(512, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x) \n",
    "\n",
    "\t\tx = Conv2D(1024, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "        \n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(2048)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization()(x)\n",
    "\t\tx = Dropout(0.5)(x)\n",
    "\t\tx = Dense(numCategories)(x)\n",
    "\t\tx = Activation(finalAct, name=\"category_output\")(x)\n",
    " \n",
    "\t\treturn x    \n",
    "    \n",
    "\t@staticmethod\n",
    "\tdef build_color_branch(inputs, numColors, finalAct=\"softmax\",\n",
    "\t\tchanDim=-1):\n",
    "\t\tx = Conv2D(16, (3, 3), padding=\"same\")(inputs)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(3, 3))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    " \n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    " \n",
    "\t\tx = Conv2D(32, (3, 3), padding=\"same\")(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\tx = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\t\tx = Dropout(0.25)(x)\n",
    "        \n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(128)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = BatchNormalization()(x)\n",
    "\t\tx = Dropout(0.5)(x)\n",
    "\t\tx = Dense(numColors)(x)\n",
    "\t\tx = Activation(finalAct, name=\"color_output\")(x)\n",
    " \n",
    "\t\treturn x\n",
    "    \n",
    "\t@staticmethod\n",
    "\tdef build(width, height, numCategories, numColors,\n",
    "\t\tfinalAct=\"softmax\"):\n",
    "\t\tinputShape = (height, width, 3)\n",
    "\t\tchanDim = -1\n",
    " \n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tcategoryBranch = FashionNet.build_category_branch(inputs,\n",
    "\t\t\tnumCategories, finalAct=finalAct, chanDim=chanDim)\n",
    "\t\tcolorBranch = FashionNet.build_color_branch(inputs,\n",
    "\t\t\tnumColors, finalAct=finalAct, chanDim=chanDim)\n",
    "\n",
    "\t\tmodel = Model(\n",
    "\t\t\tinputs=inputs,\n",
    "\t\t\toutputs=[categoryBranch, colorBranch],\n",
    "\t\t\tname=\"fashionnet\")\n",
    " \n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The colour branch is built very similarly to the category branch with the exception that the colour branch is a lot shallower. The reason for this is that the problem the colour branch needs to solve is a lot simpler than determining the category. Note as well that this time we do not apply a Lambda layer as colour information needs to be retained for processing. \n",
    "\n",
    "The final step for our network is to pull the two branches together. The result is a build function that accepts 5 parameters on instantiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue by training our model with our prepared dataset. Start by importing the needed packages for this process. We will set matplotlib backend so that accuracy can be saved on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will initialize important variables to train the model. Tweak these variables to modify its performance and how long it will take to train. 20 Epochs should be enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "INIT_LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 30\n",
    "IMAGE_DIMENSIONS = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also initialize some constants to hold the folder and file paths in our working directory. The model and binarized class lables will be stored in the relative output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FOLDER = \"data/\"\n",
    "OUTPUT_FOLDER = \"multi_branch_output/\"\n",
    "MODEL_FILE = \"vehicle_classification.model\"\n",
    "CATEGORY_LABELS_FILE = \"category_labels.pickle\"\n",
    "COLOUR_LABELS_FILE = \"color_labels.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need variables to hold our image and label information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "categoryLabels = []\n",
    "colorLabels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"IMAGE-PREPROCESSING\">\n",
    "IMAGE PREPROCESSING\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is to cycle through all image file paths within our input data folder and add them to a list. The list of file paths is then shuffled randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n"
     ]
    }
   ],
   "source": [
    "imagePaths = []\n",
    "\n",
    "for dir_, _, files in os.walk(INPUT_DATA_FOLDER):\n",
    "    for fileName in files:\n",
    "        relDir = os.path.relpath(dir_, INPUT_DATA_FOLDER)\n",
    "        relFile = os.path.join(relDir, fileName)\n",
    "        if fileName is not None:\n",
    "            imagePaths.append(relFile)\n",
    "           \n",
    "        \n",
    "random.seed(43)\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are then pre-processed by resizing them to the dimensions necessitated as input by our model. We will also convert the images to an array using a scitkit learn method. The image data is recorded along with the true class labels, each in their own separate array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagePath in imagePaths:\n",
    "        imagePath = INPUT_DATA_FOLDER + imagePath         \n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        if  image is not None:        \n",
    "            image = cv2.resize(image, (IMAGE_DIMENSIONS[1], IMAGE_DIMENSIONS[0]))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = img_to_array(image)\n",
    "            data.append(image)\n",
    "\n",
    "            (color, cat) = imagePath.split(os.path.sep)[-2].split(\"_\")           \n",
    "            categoryLabels.append(cat)\n",
    "            colorLabels.append(color) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print an element in our 3 arrays to confirm that the labels are being saved accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "black_sedan\\00000328.jpg\n",
      "sedan\n",
      "black\n"
     ]
    }
   ],
   "source": [
    "print(imagePaths[7])\n",
    "print(categoryLabels[7])\n",
    "print(colorLabels[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"LABEL-BINARIZATION\">\n",
    "LABEL BINARIZATION\n",
    "</div>\n",
    "\n",
    "Our next step is to convert the lists to numpy arrays and binaqrize the labels independently. In our last example we used scikit-learn's MultiLabelBinarizer however it is not need in this approach because outputs are being calculated sepately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] data matrix: 5667 images (1224.07MB)\n",
      "[INFO] binarizing labels...\n"
     ]
    }
   ],
   "source": [
    "# scale the raw pixel intensities to the range [0, 1] and convert to\n",
    "# a NumPy array\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    " \n",
    "categoryLabels = np.array(categoryLabels)\n",
    "colorLabels = np.array(colorLabels)\n",
    " \n",
    "categoryLB = LabelBinarizer()\n",
    "colorLB = LabelBinarizer()\n",
    "\n",
    "categoryLabels = categoryLB.fit_transform(categoryLabels)\n",
    "colorLabels = colorLB.fit_transform(colorLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also important is to split the data into distinct training and test sets. For our case 80% will be used for training with the remaining 20% as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = train_test_split(data, categoryLabels, colorLabels, test_size=0.2, random_state=42)\n",
    "(trainX, testX, trainCategoryY, testCategoryY, trainColorY, testColorY) = split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print our class labels ensure everything has been saved accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colorLB.classes_)\n",
    "print(categoryLB.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"TRAIN-THE-MODEL\">\n",
    "TRAIN THE MODEL\n",
    "</div>\n",
    "\n",
    "To train our model we first must instantiate it with the necessary parameters. For this exercise we can use the 'softmax' function as the final activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Train on 4533 samples, validate on 1134 samples\n",
      "Epoch 1/50\n",
      "4533/4533 [==============================] - 14s 3ms/step - loss: 2.2670 - category_output_loss: 1.4254 - color_output_loss: 0.8415 - category_output_acc: 0.5213 - color_output_acc: 0.6980 - val_loss: 3.6549 - val_category_output_loss: 3.1197 - val_color_output_loss: 0.5351 - val_category_output_acc: 0.3836 - val_color_output_acc: 0.7981\n",
      "Epoch 2/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 1.4655 - category_output_loss: 0.9367 - color_output_loss: 0.5288 - category_output_acc: 0.6256 - color_output_acc: 0.8028 - val_loss: 2.0933 - val_category_output_loss: 1.4452 - val_color_output_loss: 0.6481 - val_category_output_acc: 0.6367 - val_color_output_acc: 0.7619\n",
      "Epoch 3/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 1.2579 - category_output_loss: 0.8009 - color_output_loss: 0.4570 - category_output_acc: 0.6929 - color_output_acc: 0.8213 - val_loss: 2.0528 - val_category_output_loss: 1.6236 - val_color_output_loss: 0.4292 - val_category_output_acc: 0.5626 - val_color_output_acc: 0.8298\n",
      "Epoch 4/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 1.0992 - category_output_loss: 0.6696 - color_output_loss: 0.4296 - category_output_acc: 0.7441 - color_output_acc: 0.8427 - val_loss: 1.9214 - val_category_output_loss: 1.4410 - val_color_output_loss: 0.4804 - val_category_output_acc: 0.6490 - val_color_output_acc: 0.8210\n",
      "Epoch 5/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 1.0004 - category_output_loss: 0.6015 - color_output_loss: 0.3989 - category_output_acc: 0.7640 - color_output_acc: 0.8476 - val_loss: 1.1732 - val_category_output_loss: 0.7143 - val_color_output_loss: 0.4590 - val_category_output_acc: 0.7734 - val_color_output_acc: 0.8457\n",
      "Epoch 6/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.9162 - category_output_loss: 0.5508 - color_output_loss: 0.3655 - category_output_acc: 0.7803 - color_output_acc: 0.8601 - val_loss: 1.0129 - val_category_output_loss: 0.5711 - val_color_output_loss: 0.4419 - val_category_output_acc: 0.7875 - val_color_output_acc: 0.8307\n",
      "Epoch 7/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.8642 - category_output_loss: 0.5019 - color_output_loss: 0.3623 - category_output_acc: 0.8076 - color_output_acc: 0.8698 - val_loss: 1.2026 - val_category_output_loss: 0.7625 - val_color_output_loss: 0.4401 - val_category_output_acc: 0.7090 - val_color_output_acc: 0.8325\n",
      "Epoch 8/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.8034 - category_output_loss: 0.4531 - color_output_loss: 0.3503 - category_output_acc: 0.8319 - color_output_acc: 0.8703 - val_loss: 0.9491 - val_category_output_loss: 0.5512 - val_color_output_loss: 0.3979 - val_category_output_acc: 0.7795 - val_color_output_acc: 0.8607\n",
      "Epoch 9/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.7439 - category_output_loss: 0.4150 - color_output_loss: 0.3289 - category_output_acc: 0.8341 - color_output_acc: 0.8811 - val_loss: 0.8969 - val_category_output_loss: 0.4949 - val_color_output_loss: 0.4020 - val_category_output_acc: 0.8166 - val_color_output_acc: 0.8563\n",
      "Epoch 10/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.7142 - category_output_loss: 0.3937 - color_output_loss: 0.3205 - category_output_acc: 0.8513 - color_output_acc: 0.8787 - val_loss: 0.9107 - val_category_output_loss: 0.4502 - val_color_output_loss: 0.4605 - val_category_output_acc: 0.8386 - val_color_output_acc: 0.8580\n",
      "Epoch 11/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.6671 - category_output_loss: 0.3519 - color_output_loss: 0.3152 - category_output_acc: 0.8615 - color_output_acc: 0.8848 - val_loss: 0.9807 - val_category_output_loss: 0.6245 - val_color_output_loss: 0.3561 - val_category_output_acc: 0.8113 - val_color_output_acc: 0.8721\n",
      "Epoch 12/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.6262 - category_output_loss: 0.3294 - color_output_loss: 0.2969 - category_output_acc: 0.8709 - color_output_acc: 0.8884 - val_loss: 0.8513 - val_category_output_loss: 0.4295 - val_color_output_loss: 0.4218 - val_category_output_acc: 0.8536 - val_color_output_acc: 0.8519\n",
      "Epoch 13/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.5732 - category_output_loss: 0.2937 - color_output_loss: 0.2795 - category_output_acc: 0.8846 - color_output_acc: 0.8998 - val_loss: 0.9686 - val_category_output_loss: 0.6037 - val_color_output_loss: 0.3648 - val_category_output_acc: 0.7928 - val_color_output_acc: 0.8801\n",
      "Epoch 14/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.5318 - category_output_loss: 0.2618 - color_output_loss: 0.2700 - category_output_acc: 0.8983 - color_output_acc: 0.8996 - val_loss: 0.8397 - val_category_output_loss: 0.4693 - val_color_output_loss: 0.3704 - val_category_output_acc: 0.8554 - val_color_output_acc: 0.8810\n",
      "Epoch 15/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.5233 - category_output_loss: 0.2578 - color_output_loss: 0.2654 - category_output_acc: 0.9040 - color_output_acc: 0.9038 - val_loss: 1.0722 - val_category_output_loss: 0.6875 - val_color_output_loss: 0.3847 - val_category_output_acc: 0.7751 - val_color_output_acc: 0.8765\n",
      "Epoch 16/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.4991 - category_output_loss: 0.2431 - color_output_loss: 0.2560 - category_output_acc: 0.9133 - color_output_acc: 0.9034 - val_loss: 0.9660 - val_category_output_loss: 0.5885 - val_color_output_loss: 0.3775 - val_category_output_acc: 0.8201 - val_color_output_acc: 0.8757\n",
      "Epoch 17/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.5163 - category_output_loss: 0.2646 - color_output_loss: 0.2517 - category_output_acc: 0.9062 - color_output_acc: 0.9087 - val_loss: 1.1460 - val_category_output_loss: 0.5013 - val_color_output_loss: 0.6447 - val_category_output_acc: 0.8377 - val_color_output_acc: 0.8113\n",
      "Epoch 18/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.4535 - category_output_loss: 0.2144 - color_output_loss: 0.2392 - category_output_acc: 0.9175 - color_output_acc: 0.9135 - val_loss: 1.1259 - val_category_output_loss: 0.5631 - val_color_output_loss: 0.5627 - val_category_output_acc: 0.8263 - val_color_output_acc: 0.8325\n",
      "Epoch 19/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.4450 - category_output_loss: 0.2096 - color_output_loss: 0.2353 - category_output_acc: 0.9232 - color_output_acc: 0.9135 - val_loss: 1.1717 - val_category_output_loss: 0.7997 - val_color_output_loss: 0.3719 - val_category_output_acc: 0.7875 - val_color_output_acc: 0.8818\n",
      "Epoch 20/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.4397 - category_output_loss: 0.2076 - color_output_loss: 0.2321 - category_output_acc: 0.9246 - color_output_acc: 0.9100 - val_loss: 1.1730 - val_category_output_loss: 0.7704 - val_color_output_loss: 0.4026 - val_category_output_acc: 0.8007 - val_color_output_acc: 0.8686\n",
      "Epoch 21/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3784 - category_output_loss: 0.1597 - color_output_loss: 0.2186 - category_output_acc: 0.9398 - color_output_acc: 0.9193 - val_loss: 1.1601 - val_category_output_loss: 0.7282 - val_color_output_loss: 0.4319 - val_category_output_acc: 0.8131 - val_color_output_acc: 0.8686\n",
      "Epoch 22/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3670 - category_output_loss: 0.1616 - color_output_loss: 0.2054 - category_output_acc: 0.9422 - color_output_acc: 0.9219 - val_loss: 0.8889 - val_category_output_loss: 0.4943 - val_color_output_loss: 0.3946 - val_category_output_acc: 0.8642 - val_color_output_acc: 0.8713\n",
      "Epoch 23/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3661 - category_output_loss: 0.1595 - color_output_loss: 0.2066 - category_output_acc: 0.9453 - color_output_acc: 0.9215 - val_loss: 1.0560 - val_category_output_loss: 0.6841 - val_color_output_loss: 0.3718 - val_category_output_acc: 0.8175 - val_color_output_acc: 0.8845\n",
      "Epoch 24/50\n",
      "4533/4533 [==============================] - 9s 2ms/step - loss: 0.3661 - category_output_loss: 0.1640 - color_output_loss: 0.2021 - category_output_acc: 0.9391 - color_output_acc: 0.9252 - val_loss: 1.0548 - val_category_output_loss: 0.6064 - val_color_output_loss: 0.4483 - val_category_output_acc: 0.8536 - val_color_output_acc: 0.8422\n",
      "Epoch 25/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3455 - category_output_loss: 0.1504 - color_output_loss: 0.1951 - category_output_acc: 0.9444 - color_output_acc: 0.9270 - val_loss: 0.9440 - val_category_output_loss: 0.5547 - val_color_output_loss: 0.3893 - val_category_output_acc: 0.8810 - val_color_output_acc: 0.8836\n",
      "Epoch 26/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3612 - category_output_loss: 0.1716 - color_output_loss: 0.1896 - category_output_acc: 0.9365 - color_output_acc: 0.9310 - val_loss: 0.8072 - val_category_output_loss: 0.3937 - val_color_output_loss: 0.4135 - val_category_output_acc: 0.8739 - val_color_output_acc: 0.8774\n",
      "Epoch 27/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3361 - category_output_loss: 0.1505 - color_output_loss: 0.1856 - category_output_acc: 0.9479 - color_output_acc: 0.9279 - val_loss: 0.8680 - val_category_output_loss: 0.4925 - val_color_output_loss: 0.3755 - val_category_output_acc: 0.8430 - val_color_output_acc: 0.8765\n",
      "Epoch 28/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.3097 - category_output_loss: 0.1244 - color_output_loss: 0.1852 - category_output_acc: 0.9539 - color_output_acc: 0.9318 - val_loss: 0.9735 - val_category_output_loss: 0.5793 - val_color_output_loss: 0.3942 - val_category_output_acc: 0.8333 - val_color_output_acc: 0.8880\n",
      "Epoch 29/50\n",
      "4533/4533 [==============================] - 9s 2ms/step - loss: 0.2948 - category_output_loss: 0.1052 - color_output_loss: 0.1897 - category_output_acc: 0.9621 - color_output_acc: 0.9298 - val_loss: 0.8916 - val_category_output_loss: 0.4784 - val_color_output_loss: 0.4133 - val_category_output_acc: 0.8668 - val_color_output_acc: 0.8616\n",
      "Epoch 30/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2684 - category_output_loss: 0.0993 - color_output_loss: 0.1691 - category_output_acc: 0.9632 - color_output_acc: 0.9369 - val_loss: 0.9715 - val_category_output_loss: 0.5617 - val_color_output_loss: 0.4098 - val_category_output_acc: 0.8792 - val_color_output_acc: 0.8810\n",
      "Epoch 31/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2900 - category_output_loss: 0.1184 - color_output_loss: 0.1716 - category_output_acc: 0.9561 - color_output_acc: 0.9365 - val_loss: 0.8406 - val_category_output_loss: 0.4371 - val_color_output_loss: 0.4035 - val_category_output_acc: 0.8765 - val_color_output_acc: 0.8915\n",
      "Epoch 32/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2716 - category_output_loss: 0.1109 - color_output_loss: 0.1606 - category_output_acc: 0.9607 - color_output_acc: 0.9415 - val_loss: 1.1059 - val_category_output_loss: 0.6922 - val_color_output_loss: 0.4137 - val_category_output_acc: 0.8325 - val_color_output_acc: 0.8871\n",
      "Epoch 33/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2571 - category_output_loss: 0.0862 - color_output_loss: 0.1709 - category_output_acc: 0.9713 - color_output_acc: 0.9429 - val_loss: 1.1810 - val_category_output_loss: 0.7616 - val_color_output_loss: 0.4194 - val_category_output_acc: 0.8439 - val_color_output_acc: 0.8845\n",
      "Epoch 34/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2703 - category_output_loss: 0.1225 - color_output_loss: 0.1478 - category_output_acc: 0.9574 - color_output_acc: 0.9442 - val_loss: 1.0090 - val_category_output_loss: 0.6102 - val_color_output_loss: 0.3987 - val_category_output_acc: 0.8695 - val_color_output_acc: 0.8898\n",
      "Epoch 35/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2650 - category_output_loss: 0.1207 - color_output_loss: 0.1442 - category_output_acc: 0.9583 - color_output_acc: 0.9444 - val_loss: 0.8138 - val_category_output_loss: 0.4513 - val_color_output_loss: 0.3625 - val_category_output_acc: 0.8730 - val_color_output_acc: 0.8801\n",
      "Epoch 36/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2486 - category_output_loss: 0.0969 - color_output_loss: 0.1517 - category_output_acc: 0.9665 - color_output_acc: 0.9435 - val_loss: 0.9551 - val_category_output_loss: 0.5137 - val_color_output_loss: 0.4414 - val_category_output_acc: 0.8713 - val_color_output_acc: 0.8730\n",
      "Epoch 37/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2469 - category_output_loss: 0.1032 - color_output_loss: 0.1436 - category_output_acc: 0.9654 - color_output_acc: 0.9468 - val_loss: 1.0106 - val_category_output_loss: 0.5607 - val_color_output_loss: 0.4499 - val_category_output_acc: 0.8607 - val_color_output_acc: 0.8739\n",
      "Epoch 38/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2209 - category_output_loss: 0.0843 - color_output_loss: 0.1366 - category_output_acc: 0.9698 - color_output_acc: 0.9535 - val_loss: 2.1905 - val_category_output_loss: 1.7967 - val_color_output_loss: 0.3938 - val_category_output_acc: 0.6667 - val_color_output_acc: 0.8907\n",
      "Epoch 39/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2260 - category_output_loss: 0.0952 - color_output_loss: 0.1308 - category_output_acc: 0.9689 - color_output_acc: 0.9526 - val_loss: 1.6102 - val_category_output_loss: 1.0750 - val_color_output_loss: 0.5352 - val_category_output_acc: 0.7866 - val_color_output_acc: 0.8660\n",
      "Epoch 40/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2328 - category_output_loss: 0.1006 - color_output_loss: 0.1322 - category_output_acc: 0.9645 - color_output_acc: 0.9504 - val_loss: 0.9171 - val_category_output_loss: 0.4842 - val_color_output_loss: 0.4329 - val_category_output_acc: 0.8827 - val_color_output_acc: 0.8704\n",
      "Epoch 41/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2115 - category_output_loss: 0.0844 - color_output_loss: 0.1272 - category_output_acc: 0.9731 - color_output_acc: 0.9526 - val_loss: 0.9041 - val_category_output_loss: 0.4967 - val_color_output_loss: 0.4074 - val_category_output_acc: 0.8871 - val_color_output_acc: 0.8915\n",
      "Epoch 42/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2303 - category_output_loss: 0.1037 - color_output_loss: 0.1266 - category_output_acc: 0.9621 - color_output_acc: 0.9546 - val_loss: 1.0358 - val_category_output_loss: 0.6212 - val_color_output_loss: 0.4147 - val_category_output_acc: 0.8624 - val_color_output_acc: 0.8862\n",
      "Epoch 43/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2060 - category_output_loss: 0.0724 - color_output_loss: 0.1336 - category_output_acc: 0.9737 - color_output_acc: 0.9475 - val_loss: 0.9206 - val_category_output_loss: 0.4940 - val_color_output_loss: 0.4266 - val_category_output_acc: 0.8765 - val_color_output_acc: 0.8845\n",
      "Epoch 44/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.1926 - category_output_loss: 0.0750 - color_output_loss: 0.1176 - category_output_acc: 0.9753 - color_output_acc: 0.9572 - val_loss: 1.0652 - val_category_output_loss: 0.5968 - val_color_output_loss: 0.4685 - val_category_output_acc: 0.8545 - val_color_output_acc: 0.8845\n",
      "Epoch 45/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.1996 - category_output_loss: 0.0825 - color_output_loss: 0.1171 - category_output_acc: 0.9746 - color_output_acc: 0.9590 - val_loss: 0.9935 - val_category_output_loss: 0.5280 - val_color_output_loss: 0.4656 - val_category_output_acc: 0.8686 - val_color_output_acc: 0.8721output_loss: 0.1128 - cat\n",
      "Epoch 46/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2093 - category_output_loss: 0.0967 - color_output_loss: 0.1126 - category_output_acc: 0.9682 - color_output_acc: 0.9581 - val_loss: 1.2129 - val_category_output_loss: 0.7161 - val_color_output_loss: 0.4968 - val_category_output_acc: 0.7910 - val_color_output_acc: 0.8765\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.2041 - category_output_loss: 0.1005 - color_output_loss: 0.1037 - category_output_acc: 0.9680 - color_output_acc: 0.9621 - val_loss: 0.9821 - val_category_output_loss: 0.5779 - val_color_output_loss: 0.4042 - val_category_output_acc: 0.8810 - val_color_output_acc: 0.8889\n",
      "Epoch 48/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.1893 - category_output_loss: 0.0692 - color_output_loss: 0.1201 - category_output_acc: 0.9760 - color_output_acc: 0.9579 - val_loss: 1.0159 - val_category_output_loss: 0.5388 - val_color_output_loss: 0.4771 - val_category_output_acc: 0.8845 - val_color_output_acc: 0.8845\n",
      "Epoch 49/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.1833 - category_output_loss: 0.0714 - color_output_loss: 0.1119 - category_output_acc: 0.9737 - color_output_acc: 0.9568 - val_loss: 0.8748 - val_category_output_loss: 0.4880 - val_color_output_loss: 0.3867 - val_category_output_acc: 0.8721 - val_color_output_acc: 0.8898\n",
      "Epoch 50/50\n",
      "4533/4533 [==============================] - 10s 2ms/step - loss: 0.1880 - category_output_loss: 0.0859 - color_output_loss: 0.1022 - category_output_acc: 0.9726 - color_output_acc: 0.9625 - val_loss: 1.0657 - val_category_output_loss: 0.5125 - val_color_output_loss: 0.5532 - val_category_output_acc: 0.8862 - val_color_output_acc: 0.8757\n",
      "[INFO] serializing network...\n"
     ]
    }
   ],
   "source": [
    "model = VehicleNet.build(IMAGE_DIMENSIONS[1], IMAGE_DIMENSIONS[1],\n",
    "\tnumCategories=len(categoryLB.classes_),\n",
    "\tnumColors=len(colorLB.classes_),\n",
    "\tfinalAct=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key to this solution to the multiple classification problem is to create two losses for each of the separate branches. Loss weights will also be defined separately and will allow for independent modification. Try changing these values and see how it affects you model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\n",
    "\t\"category_output\": \"categorical_crossentropy\",\n",
    "\t\"color_output\": \"categorical_crossentropy\",\n",
    "}\n",
    "lossWeights = {\"category_output\": 1.0, \"color_output\": 1.0} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to initialize the optimizer and compile the model using our previously defined metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(lr=INIT_LEARNING_RATE, decay=INIT_LEARNING_RATE / EPOCHS)\n",
    "model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights,\n",
    "\tmetrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model using it's fit() method. Feed it the appropriate training and test sets of data. Afterwards we can save the model to our output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit(trainX,\n",
    "\t{\"category_output\": trainCategoryY, \"color_output\": trainColorY},\n",
    "\tvalidation_data=(testX,\n",
    "\t\t{\"category_output\": testCategoryY, \"color_output\": testColorY}),\n",
    "\tepochs=EPOCHS,\n",
    "\tverbose=1)\n",
    "\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(OUTPUT_FOLDER + MODEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to save our two binarized labels in separate files, these will also be sent to our output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing category label binarizer...\n",
      "[INFO] serializing color label binarizer...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] serializing category label binarizer...\")\n",
    "f = open(OUTPUT_FOLDER + CATEGORY_LABELS_FILE, \"wb\")\n",
    "f.write(pickle.dumps(categoryLB))\n",
    "f.close()\n",
    " \n",
    "print(\"[INFO] serializing color label binarizer...\")\n",
    "f = open(OUTPUT_FOLDER + COLOUR_LABELS_FILE, \"wb\")\n",
    "f.write(pickle.dumps(colorLB))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"ACCURACY-STATISTICS\">\n",
    "ACCURACY STATISTICS\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again as a last step in the training process we can visualize the accuracy of our model by plotting its statistics over the number of epochs. The val_output_acc  figures represent the accuracy on our testing split of data. The graph will be saved as a png file to our output folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracyNames = [\"category_output_acc\", \"color_output_acc\"]\n",
    "plt.style.use(\"ggplot\")\n",
    "(fig, ax) = plt.subplots(2, 1, figsize=(8, 8))\n",
    " \n",
    "for (i, l) in enumerate(accuracyNames):\n",
    "\t# plot the loss for both the training and validation data\n",
    "\tax[i].set_title(\"Accuracy for {}\".format(l))\n",
    "\tax[i].set_xlabel(\"Epoch #\")\n",
    "\tax[i].set_ylabel(\"Accuracy\")\n",
    "\tax[i].plot(np.arange(0, EPOCHS), H.history[l], label=l)\n",
    "\tax[i].plot(np.arange(0, EPOCHS), H.history[\"validation_\" + l],\n",
    "\t\tlabel=\"val_\" + l)\n",
    "\tax[i].legend()\n",
    " \n",
    "# save the accuracies figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_FOLDER + \"{}_accs.png\".format(\"output\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"IMPLEMENTATION\">\n",
    "IMPLEMENTATION\n",
    "</div>\n",
    "\n",
    "We will now test our model’s performance on a set of images with class combinations it has not explicitly seen during training. We can then compare it to our previous exercise and determine which approach to the multiple output problem is most effective. \n",
    "\n",
    "Let's start by importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add some constants that will hold paths in our working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DATA_FOLDER = \"unseen_class_combinations/\"\n",
    "RESULTS_FOLDERS = [\"correct_predictions/\", \"incorrect_predictions/\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reset the image pre-processing variables so we can reuse them in our test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we trained our model we will loop over every file within the given directory and add its file path to an array and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] loading images...\")\n",
    "imagePaths = []\n",
    "\n",
    "for dir_, _, files in os.walk(INPUT_DATA_FOLDER):\n",
    "    for fileName in files:\n",
    "        relDir = os.path.relpath(dir_, INPUT_DATA_FOLDER)\n",
    "        relFile = os.path.join(relDir, fileName)\n",
    "        if fileName is not None:\n",
    "            imagePaths.append(relFile)\n",
    "           \n",
    "        \n",
    "random.seed(43)\n",
    "random.shuffle(imagePaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every image stored in our file paths array we will pre-process the image and keep track of its true classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imagePath in imagePaths:\n",
    "        imagePath = INPUT_DATA_FOLDER + imagePath         \n",
    "        image = cv2.imread(imagePath)\n",
    "        \n",
    "        if  image is not None:    \n",
    "            image = cv2.resize(image, (IMAGE_DIMENSIONS[1], IMAGE_DIMENSIONS[0]))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = img_to_array(image)\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            data.append(image)\n",
    "            \n",
    "            labels.append(imagePath) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load our saved model in addition to the two binarized label files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(OUTPUT_FOLDER + MODEL_FILE,custom_objects={\"tf\": tf})\n",
    "\n",
    "categoryLB = pickle.loads(open(OUTPUT_FOLDER + CATEGORY_LABELS_FILE, \"rb\").read())\n",
    "colorLB = pickle.loads(open(OUTPUT_FOLDER + COLOUR_LABELS_FILE, \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we test an image with our model we will overlay its prediction in text over a copy of the image and then save it to the correct folder based on if the image has been accurately classified or not. For the sake of neatness we will take a second below to clear the contents of these folders for a fresh start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in RESULTS_FOLDERS:\n",
    "    for the_file in os.listdir(OUTPUT_FOLDER + i):\n",
    "        file_path = os.path.join(OUTPUT_FOLDER + i, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now cycle through each image in our unseen class combinations folder. We will refer to our binarized labels to extract the predicted classes for each image. This is done by finding the index for the largest probability of each image given our model’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for (images, lab) in zip(data, labels):\n",
    "    (categoryProba, colorProba) = model.predict(images)\n",
    "    \n",
    "    categoryIdx = categoryProba[0].argmax()\n",
    "    colorIdx = colorProba[0].argmax()\n",
    "   \n",
    "    categoryLabel = categoryLB.classes_[categoryIdx]\n",
    "    colorLabel = colorLB.classes_[colorIdx]\n",
    "\n",
    "    categoryText = \"category: {} ({:.2f}%)\".format(categoryLabel, categoryProba[0][categoryIdx] * 100)\n",
    "    colorText = \"color: {} ({:.2f}%)\".format(colorLabel, colorProba[0][colorIdx] * 100)\n",
    "\n",
    "    image = cv2.imread(lab)\n",
    "    \n",
    "    output = imutils.resize(image, width=1000)\n",
    "\n",
    "    (actual_colour, actual_category) = lab.split(os.path.sep)[-2].split(\"_\")\n",
    "     \n",
    "    cv2.putText(output, categoryText, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7, (0, 255, 0), 2)\n",
    "    cv2.putText(output, colorText, (10, 55), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.7, (0, 255, 0), 2)\n",
    "\n",
    "    if actual_category == categoryLabel :\n",
    "        if actual_colour == colorLabel:\n",
    "            cv2.imwrite(OUTPUT_FOLDER + RESULTS_FOLDERS[0] + str(counter) + '.jpg', output)\n",
    "        elif actual_colour != colorLabel:\n",
    "            cv2.imwrite(OUTPUT_FOLDER + RESULTS_FOLDERS[1]+ str(counter) + '.jpg', output)\n",
    "    elif actual_category != categoryLabel:\n",
    "        cv2.imwrite(OUTPUT_FOLDER + RESULTS_FOLDERS[1]+ str(counter) + '.jpg', output)\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we count the number of images that were correctly identified by examining the images saved in its folder. A figure for the overall accuracy on our test set of images can also be calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = len(imagePaths)\n",
    "\n",
    "list = os.listdir(OUTPUT_FOLDER + RESULTS_FOLDERS[0]) # dir is your directory path\n",
    "number_files = len(list)\n",
    "\n",
    "unseen_accuracy =  number_files/total_files * 100\n",
    "\n",
    "print ('The prdiction accuracy for unseen combinations: %' + str(unseen_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#0B8261; width:100%; height:38px; color:white; font-size:18px; padding:10px;\" id=\"CONCLUSION\">\n",
    "CONCLUSION\n",
    "</div>\n",
    "\n",
    "Remember that the images we used to implement this model specifically contained objects whose class combinations had not been seen during training. By reviewing the folder that contains successfully classified images we can see that the approach taken in this exercise did indeed accurately classify both outputs for a number of our test images. Even if our overall accuracy percentage on our test set is not that impressive, it is important to note that when compared with the multi-binarized-label approach of our previous exercise it is apparent that this technique vastly out performs it. Although our exercise may not be the perfect execution of multiple branch architecture, the take away is that this approach should be the focus of refinement if one were to seek maximum classification accuracy on a multiple output problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
